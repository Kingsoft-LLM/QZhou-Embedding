---
license: apache-2.0
tags:
  - sentence-transformers
  - sentence-similarity
  - mteb
  - retriever
  - text-embeddings-inference
---
# QZhou-Embedding
<div align="center">
<img src="assets/image-1.png" width="800" height="300"></img>
</div>

## ç®€ä»‹
æˆ‘ä»¬å‘å¸ƒ<a href="https://huggingface.co/Kingsoft-LLM/QZhou-Embedding">QZhou-Embedding</a>(è½»èˆŸEmbeddingğŸ˜ˆğŸ˜ˆğŸ˜ˆ)ï¼Œé¢å‘é€šç”¨é¢†åŸŸçš„æ–‡æœ¬å‘é‡è¡¨ç¤ºå¤§æ¨¡å‹ï¼Œæ“…é•¿å„ç§æ–‡æœ¬åµŒå…¥ï¼ˆæ£€ç´¢ã€é‡æ’ã€å¥å¯¹ç›¸ä¼¼åº¦ã€åˆ†ç±»ï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬åŸºäºQwen2Modelæ”¹é€ æ¨¡å‹é€»è¾‘ï¼Œå°†å› æœæ³¨æ„åŠ›æœºåˆ¶ä¿®æ”¹ä¸ºåŒå‘æ³¨æ„åŠ›ï¼Œä½¿æ¯ä¸ªtokenéƒ½èƒ½æ•è·åˆ°å…¨å±€ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œæ–°æ¨¡å—å‘½åä¸ºQZhouModelã€‚å¾—ç›ŠäºåŸºç¡€æ¨¡å‹åœ¨æµ·é‡æ–‡æœ¬ä¸Šé¢„è®­ç»ƒè·å¾—çš„é€šç”¨è¯­è¨€èƒ½åŠ›ï¼ŒQZhou-Embeddingèƒ½å¤Ÿè·å¾—æ›´åŠ å¼ºå¤§çš„æ–‡æœ¬åµŒå…¥è¡¨ç¤ºã€‚QZhou-Embeddingä½¿ç”¨ç™¾ä¸‡é‡çº§é«˜è´¨é‡å¼€æºæ£€ç´¢æ•°æ®ï¼Œä»¥åŠ500ä¸‡+é«˜è´¨é‡åˆæˆæ•°æ®ï¼ˆæ”¹å†™ã€æ‰©å±•ä¸¤å¤§åˆæˆæŠ€æœ¯ï¼‰è¿›è¡ŒæŒç»­è®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡ç¬¬ä¸€é˜¶æ®µæ£€ç´¢è®­ç»ƒä¸ºæ¨¡å‹æä¾›query-docè¯­ä¹‰åŒ¹é…èƒ½åŠ›åŸºç¡€ï¼Œç¬¬äºŒé˜¶æ®µçš„STSã€èšç±»ç­‰å¤šç»´åº¦èƒ½åŠ›è®­ç»ƒå¸®åŠ©æ¨¡å‹åœ¨å„ç§åœºæ™¯ä¸‹æŒç»­çªç ´ã€‚QZhou-Embeddingçš„æ¨¡å‹å‚æ•°ä¸º7Bï¼Œå…·å¤‡æœ€å¤§8kçš„é•¿æ–‡æœ¬å‘é‡åµŒå…¥èƒ½åŠ›ã€‚åœ¨mteb/cmtebè¯„æµ‹åŸºå‡†ä¸Šå–å¾—å‡å€¼å…¨æ¦œæœ€é«˜ï¼Œå„ä»»åŠ¡æŒ‡æ ‡æ–¹é¢ï¼Œèšç±»ã€å¥å¯¹åˆ†ç±»ã€é‡æ’ã€STSä»»åŠ¡æŒ‡æ ‡å‡å€¼å…¨æ¦œæœ€é«˜çš„æ•ˆæœã€‚

## QZhou-EmbeddingåŸºæœ¬ç‰¹ç‚¹

- å¼ºå¤§çš„æ–‡æœ¬åµŒå…¥èƒ½åŠ›ï¼›
- é•¿ä¸Šä¸‹æ–‡ï¼šæœ€å¤§æ”¯æŒ8kï¼›
- å‚æ•°é‡7B


## æŠ€æœ¯ä»‹ç»
### ç»Ÿä¸€ä»»åŠ¡å»ºæ¨¡æ¡†æ¶
å°†æ–‡æœ¬åµŒå…¥ç›®æ ‡ç»Ÿä¸€ä¸ºä¸‰å¤§é—®é¢˜å»ºæ¨¡ä¼˜åŒ–ï¼Œæå‡ºç»Ÿä¸€çš„è®­ç»ƒæ•°æ®ç»“æ„åŒ–æ–¹æ¡ˆå’Œå¯¹åº”çš„è®­ç»ƒæœºåˆ¶---å¯èå…¥å¤§éƒ¨åˆ†å¼€æºæ•°æ®ä½œä¸ºæ£€ç´¢è®­ç»ƒé›†ï¼Œå¯ç»“æ„åŒ–æ•°æ®å¦‚ä¸‹ï¼š
- æ£€ç´¢
  - title-body
  - title-abstract
  - é—®ç­”ç±»æ•°æ®
  - é˜…è¯»ç†è§£
  - ...

- STS
  - æ–‡æœ¬å¯¹+{true, false}ã€{yes, no}æ ‡ç­¾
  - æ–‡æœ¬å¯¹+åˆ†æ•°ï¼ˆå¦‚0.2ã€3.1ã€4.8ç­‰ï¼‰
  - NLIæ•°æ®ï¼šæ–‡æœ¬å¯¹+{'entailment', 'neutral', 'contradiction'}æ ‡ç­¾

- CLS
  - å¥å­+ç±»æ ‡ç­¾

<div align="center"><img src="assets/image-18.png" width="1000" height="600"></img></div>
<div align="center"><img src="assets/image-16.png" width="1000" height="550"></img></div>

### è®­ç»ƒç›®æ ‡

- æ£€ç´¢ï¼šä½¿ç”¨InfoNCEå¯¹æ¯”å­¦ä¹ losså‡½æ•°ï¼Œæ•ˆä»¿gte/qwen3-embeddingçš„æ”¹è¿›å¢åŠ q-qå¯¹è´Ÿæ ·ä¾‹æƒ©ç½š<br>
<div align="center"><img src="assets/formula1.png" width="700" height="110"></img></div>

- STSï¼šä½¿ç”¨Cosent lossï¼š
<div align="center"><img src="assets/formula2.png" width="700" height="110"></img></div>

- CLSï¼šåŒæ£€ç´¢ä¸€è‡´ä½¿ç”¨InfoNCE lossï¼Œä½†In-Batch Negativeæ—¶ç”±äºåŒç±»å†²çªæ¦‚ç‡å¤§ï¼Œä½¿ç”¨maskæœºåˆ¶æ©ç›–ä¸åŒæ ·æœ¬å…±äº«çš„è´Ÿæ ·ä¾‹ä¸­çš„åŒç±»æ ·æœ¬ã€‚
<div align="center"><img src="assets/formula3.png" width="1100" height="180"></img></div>
å…¶ä¸­${C_{t_i}}$è¡¨ç¤ºæ ·æœ¬${t_i}$çš„ç±»æ ‡ç­¾ï¼Œnæ˜¯å•æ¡æ•°æ®çš„è´Ÿæ ·æœ¬æ•°ã€‚

### ç‰¹å¾å¢å¼ºæ•°æ®åˆæˆæŠ€æœ¯
åœ¨å½“ä»Šå¤§æ¨¡å‹è¯­è¨€åŠåˆ›ä½œèƒ½åŠ›å¼ºå¤§çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å……åˆ†åˆ©ç”¨äº†å¤§æ¨¡å‹APIè®¾è®¡æ•°æ®åˆæˆæŠ€æœ¯ã€‚é’ˆå¯¹è®­ç»ƒé›†ä¸­å­˜åœ¨æ•°æ®å°‘ã€è¯é¢˜ç‹­éš˜ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºæ”¹å†™ã€æ‰©å±•åˆæˆæŠ€æœ¯ï¼›åŒæ—¶ä¸ºå¢å¼ºè®­ç»ƒæ—¶çš„è´Ÿæ ·ä¾‹éš¾åº¦ï¼Œæˆ‘ä»¬åœ¨ç°æœ‰åŸºäºå¼ºå¤§Embeddingå®ç°éš¾è´Ÿä¾‹é‡‡æ ·çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨åŸºäºå¤§æ¨¡å‹çš„éš¾è´Ÿæ ·ä¾‹åˆæˆæŠ€æœ¯ã€‚å‡ ç§æŠ€æœ¯ä»‹ç»å¦‚ä¸‹ï¼š
<div align="center"><img src="assets/image-9.png" width="930" height="290"></img></div>
<div align="center"><img src="assets/image-10.png" width="880" height="220"></img></div>
<div align="center"><img src="assets/image-11.png" width="880" height="210"></img></div>

æƒ³è¦è·å–æ›´å¤šä¿¡æ¯ï¼ˆå¦‚è¯„æµ‹è„šæœ¬ã€æŒ‡ä»¤æ ¼å¼ç­‰ï¼‰ï¼Œæ¬¢è¿è®¿é—®æˆ‘ä»¬çš„Githubï¼š<a href="https://github.com/Kingsoft-LLM/QZhou-Embedding">GitHub</a>

## è¯„æµ‹ç»“æœ
### mtebæ¦œå•æ˜ç»†
<div align="center"><img src="assets/image-7.png" width="1100" height="260"></img></div>

### cmtebæ¦œå•æ˜ç»†
<div align="center"><img src="assets/image-8.png" width="1000" height="260"></img></div>

## ä½¿ç”¨æŒ‡å—
### å®Œå…¨å¤ç°æ¦œå•ç»“æœ
æˆ‘ä»¬æä¾›è¯¦ç»†çš„å‚æ•°ã€ç¯å¢ƒé…ç½®ï¼Œä»¥ä¾¿èƒ½å¤Ÿåœ¨è‡ªå·±çš„æœºå™¨ä¸Šå®Œå…¨è·‘å‡ºè·Ÿæ¦œå•ä¸€è‡´çš„ç»“æœï¼ŒåŒ…æ‹¬ç¯å¢ƒä¾èµ–ã€æ¨¡å‹å‚æ•°ç­‰é…ç½®ã€‚
#### ç¯å¢ƒä¾èµ–ç‰ˆæœ¬
- Python: 3.10.12
- Sentence Transformers: 3.4.1
- Transformers: 4.51.1
- PyTorch: 2.7.1
- Accelerate: 1.3.0
- Datasets: 3.2.0
- Tokenizers: 0.21.2
- mteb: 1.38.30
#### æ¨¡å‹åŠ è½½å‚æ•°
torch_dtype=torch.bfloat16<br>
attn_implementation='sdpa'<br>
**æ³¨ï¼š** æ¦œå•ç»“æœä½¿ç”¨äº†sdpaæ¨¡å¼ï¼Œå…¶ä»–æ¨¡å¼('eager'ã€ 'flash_attention_2')å­˜åœ¨åå·®ï¼Œä½†ä¸å½±å“æ•´ä½“è¡¨ç°
#### æŒ‡ä»¤æ·»åŠ è§„åˆ™
åœ¨æˆ‘ä»¬çš„<a href="https://github.com/Kingsoft-LLM/QZhou-Embedding">GitHub</a>ä¸Šå¯ä»¥æ‰¾åˆ°ã€‚
#### è¯„æµ‹ä»£ç ä½¿ç”¨
åœ¨<a href="https://github.com/Kingsoft-LLM/QZhou-Embedding">GitHub</a>ä¸Šæ‰¾åˆ°æˆ‘ä»¬çš„è¯„æµ‹ä»£ç ï¼Œå…¶ä¸­mtebè¯„æµ‹è„šæœ¬æ˜¯**run_mteb_all_v2.py**ï¼Œcmtebè¯„æµ‹è„šæœ¬æ˜¯**run_cmteb_all.py**ï¼Œè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
```
POOLING_MODE=mean
normalize=true
use_instruction=true
export TOKENIZERS_PARALLELISM=true

model_name_or_path=æ¨¡å‹ç›®å½•ä½ç½®

python3 ./run_cmteb_all.py \
    --model_name_or_path ${model_name_or_path}  \
    --pooling_mode ${POOLING_MODE} \
    --normalize ${normalize} \
    --use_instruction ${use_instruction} \
    --output_dir ç»“æœè¾“å‡ºè·¯å¾„

python3 ./run_mteb_all_v2.py \
    --model_name_or_path ${model_name_or_path}  \
    --pooling_mode ${POOLING_MODE} \
    --normalize ${normalize} \
    --use_instruction ${use_instruction} \
    --output_dir ç»“æœè¾“å‡ºè·¯å¾„
```
è¿™æ˜¯ä¸€å¥—é€šç”¨è„šæœ¬ï¼Œå¯ä»¥ç”¨äºå…¶ä»–huggingface embeddingæ¨¡å‹çš„è¯„æµ‹ï¼Œä½†éœ€è¦ç¡®ä¿poolingç­‰é…ç½®æ­£ç¡®ã€‚

### Sentence Transformers

```
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("Kingsoft-LLM/QZhou-Embedding")

model = SentenceTransformer(
    "Kingsoft-LLM/QZhou-Embedding",
    model_kwargs={"device_map": "cuda", "trust_remote_code": True},
    tokenizer_kwargs={"padding_side": "left", "trust_remote_code": True},
    trust_remote_code=True
)

queries = [
    "What is photosynthesis?",
    "Who invented the telephone?",
]
documents = [
    "Photosynthesis is the process by which green plants use sunlight, carbon dioxide, and water to produce glucose and oxygen. This biochemical reaction occurs in chloroplasts.",
    "Alexander Graham Bell is credited with inventing the first practical telephone in 1876, receiving US patent number 174,465 for his device."
]

query_embeddings = model.encode(queries, prompt_name="query", normalize_embeddings=True)
document_embeddings = model.encode(documents, normalize_embeddings=True)

similarity = model.similarity(query_embeddings, document_embeddings)
```

### Huggingface Transformers

```
import torch
import torch.nn.functional as F

from torch import Tensor
from transformers import AutoTokenizer, AutoModel


def mean_pool(last_hidden_states: Tensor,
                 attention_mask: Tensor) -> Tensor:

    seq_lengths = attention_mask.sum(dim=-1)
    return torch.stack(
                [
                    last_hidden_states[i, -length:, :].sum(dim=0) / length
                    for i, length in enumerate(seq_lengths)
                ],
                dim=0,
            )


def get_detailed_instruct(task_description: str, query: str) -> str:
    return f'Instruct: {task_description}\nQuery:{query}'

task = 'Given a web search query, retrieve relevant passages that answer the query'

queries = [
    get_detailed_instruct(task, 'What is photosynthesis?'),
    get_detailed_instruct(task, 'Who invented the telephone?')
]

documents = [
    "Photosynthesis is the process by which green plants use sunlight, carbon dioxide, and water to produce glucose and oxygen. This biochemical reaction occurs in chloroplasts.",
    "Alexander Graham Bell is credited with inventing the first practical telephone in 1876, receiving US patent number 174,465 for his device."
]

input_texts = queries + documents

tokenizer = AutoTokenizer.from_pretrained('Kingsoft-LLM/QZhou-Embedding', padding_side='left', trust_remote_code=True)
model = AutoModel.from_pretrained('Kingsoft-LLM/QZhou-Embedding', trust_remote_code=True, device_map='cuda')

batch_dict = tokenizer(
    input_texts,
    padding=True,
    truncation=True,
    max_length=8192,
    return_tensors="pt",
)
batch_dict.to(model.device)
outputs = model(**batch_dict)
embeddings = mean_pool(outputs.last_hidden_state, batch_dict['attention_mask'])

embeddings = F.normalize(embeddings, p=2, dim=1)
scores = (embeddings[:2] @ embeddings[2:].T)
```
